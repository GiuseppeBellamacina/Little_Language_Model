{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Little Language Model\n",
    "## DanteGPT\n",
    "\n",
    "This notebook is a simple implementation of a **Transformer** architecture. The model is trained on the text of Dante's **Divina Commedia**. The model is then used to generate text."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Downloading and Saving Text Data from a URL\n",
    "\n",
    "This code snippet downloads the full text of *La Divina Commedia* by Dante Alighieri from a given URL and saves it to a local file named `divina.txt`. \n",
    "\n",
    "#### Steps Involved\n",
    "\n",
    "1. **Define the URL and File Name**:\n",
    "   - `url`: The web address pointing to the text file of *La Divina Commedia*.\n",
    "   - `file`: The name of the file where the downloaded text will be saved.\n",
    "\n",
    "2. **Downloading the File**:\n",
    "   - `requests.get(url)`: Sends an HTTP GET request to the URL, fetching the contents of the file.\n",
    "   - `response`: Stores the HTTP response, which contains the text data in its `.text` attribute.\n",
    "\n",
    "3. **Saving the Data Locally**:\n",
    "   - Opens `divina.txt` in write mode with UTF-8 encoding to ensure proper handling of special characters.\n",
    "   - Writes the contents from `response.text` into `divina.txt`.\n",
    "\n",
    "#### Example Output\n",
    "After running this code, the text of *La Divina Commedia* will be saved locally in `divina.txt`. This allows further processing or analysis of the text, such as text mining or language modeling.\n",
    "\n",
    "#### Important Notes\n",
    "- **Encoding**: UTF-8 encoding is specified to handle any Italian characters correctly.\n",
    "- **Internet Access**: Running this code requires an internet connection to download the text file from the URL.\n",
    "  \n",
    "This setup is commonly used in Natural Language Processing (NLP) projects for importing and preparing text data for analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "\n",
    "url = 'https://dmf.unicatt.it/~della/pythoncourse18/commedia.txt'\n",
    "file = 'divina.txt'\n",
    "response = requests.get(url)\n",
    "with open(file, 'w', encoding='utf-8') as f:\n",
    "    f.write(response.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preparing Text Data for Processing\n",
    "\n",
    "In this section, we load, encode, and decode the text data from *La Divina Commedia* to facilitate further analysis. \n",
    "\n",
    "#### Steps Involved\n",
    "\n",
    "1. **Loading the Text Data**:\n",
    "   - `text = open(file, 'r', encoding='utf-8').read()`: Reads the content of `divina.txt` (previously downloaded) as a single string, with UTF-8 encoding to ensure that special characters are handled correctly.\n",
    "\n",
    "2. **Creating a Vocabulary**:\n",
    "   - `vocab = sorted(list(set(text)))`: Builds a sorted list of unique characters in the text. This vocabulary will allow each character to be mapped to a unique integer index, which is essential for encoding the text.\n",
    "\n",
    "3. **Defining Encoding and Decoding Functions**:\n",
    "   - `encode = lambda s: [vocab.index(c) for c in s]`: Encodes a string `s` into a list of integers by mapping each character to its index in `vocab`.\n",
    "   - `decode = lambda l: \"\".join([vocab[c] for c in l])`: Decodes a list of integers `l` back into a string by mapping each integer index to its corresponding character in `vocab`.\n",
    "\n",
    "#### Example Usage\n",
    "\n",
    "- `encode(\"Dante\")` would produce a list of integers representing the indices of the characters in the vocabulary.\n",
    "- `decode([0, 1, 2, 3])` would convert the list of indices back into the corresponding characters.\n",
    "\n",
    "#### Importance\n",
    "\n",
    "This encoding and decoding step is essential for preparing the text data in a way that can be processed by machine learning models, such as neural networks. By working with integer indices, we can easily convert text data into a format suitable for input to a model, and we can interpret model outputs by converting indices back into readable text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = open(file, 'r', encoding='utf-8').read()\n",
    "vocab = sorted(list(set(text)))\n",
    "encode = lambda s: [vocab.index(c) for c in s]\n",
    "decode = lambda l: \"\".join([vocab[c] for c in l]) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ids: [24, 39, 45, 1, 46, 39, 58, 58, 48, 1, 38, 39, 45, 1, 37, 35, 46, 46, 43, 47, 1, 38, 43, 1, 47, 48, 52, 53, 51, 35, 1, 55, 43, 53, 35]\n",
      "txt: Nel mezzo del cammin di nostra vita\n"
     ]
    }
   ],
   "source": [
    "ids = encode(\"Nel mezzo del cammin di nostra vita\")\n",
    "txt = decode(ids)\n",
    "print(f\"ids: {ids}\")\n",
    "print(f\"txt: {txt}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Importing the necessary libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import functional as F"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Splitting the Text Data into Training and Validation Sets\n",
    "\n",
    "To prepare the data for training a model, we divide the encoded text into training and validation sets. \n",
    "\n",
    "#### Steps Involved\n",
    "\n",
    "1. **Setting the Split Point**:\n",
    "   - `x = int(0.9 * len(text))`: We set `x` to represent 90% of the length of the text. This will be the split point between the training and validation data. A 90-10 split is commonly used, allowing the model to learn from most of the data while holding back a portion to evaluate its performance.\n",
    "\n",
    "2. **Encoding and Converting to Tensor**:\n",
    "   - `text = torch.tensor(encode(text), dtype=torch.long)`: The entire text is encoded into integer indices and then converted to a PyTorch tensor of type `long`. This tensor format is efficient and compatible with PyTorch models.\n",
    "\n",
    "3. **Creating Training and Validation Sets**:\n",
    "   - `train, val = text[:x], text[x:]`: The first 90% of the text (up to index `x`) is assigned to `train`, while the remaining 10% is assigned to `val`.\n",
    "\n",
    "#### Importance\n",
    "\n",
    "Dividing the data into training and validation sets is a critical step in machine learning, as it allows us to evaluate the model’s generalization ability on unseen data. The training set is used to fit the model, while the validation set helps assess its performance and guide tuning decisions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = int(0.9*len(text))\n",
    "text = torch.tensor(encode(text), dtype=torch.long)\n",
    "train, val = text[:x], text[x:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generating Batches for Training and Validation\n",
    "\n",
    "The `get_batch` function is designed to generate small batches of input (`x`) and target (`y`) tensors, which are used to train a transformer model. The function handles both training and validation data, depending on the specified `split` parameter, and outputs tensors moved to the appropriate device (CPU or GPU).\n",
    "\n",
    "#### Key Parameters\n",
    "\n",
    "- **`batch_size`**: This defines the number of sequences processed simultaneously, enhancing parallel processing. Here, it is set to 32.\n",
    "- **`block_size`**: This represents the maximum sequence length or context window size, defining how many tokens each input sequence can see to make predictions.\n",
    "\n",
    "#### Device Selection\n",
    "\n",
    "The code detects if a GPU (`cuda`) is available and uses it if possible; otherwise, it defaults to the CPU (`cpu`). This ensures compatibility with different hardware setups.\n",
    "\n",
    "#### Function Details\n",
    "\n",
    "The `get_batch` function takes a `split` parameter, which specifies whether to generate data from the training or validation set. Here's a breakdown of the process:\n",
    "\n",
    "1. **Data Selection**: `data` is assigned to either the `train` or `val` dataset based on the specified split.\n",
    "2. **Random Sequence Selection**: A tensor `ix` of random starting indices is generated. These indices mark the starting points for each sequence in the batch.\n",
    "3. **Input and Target Tensors**:\n",
    "   - `x`: For each index `i` in `ix`, a block of `block_size` tokens starting at `i` is selected.\n",
    "   - `y`: Target tokens are created by shifting the `x` sequence one position forward, allowing the model to predict the next token in the sequence.\n",
    "4. **Device Assignment**: Both `x` and `y` are moved to the specified device, making them ready for model input.\n",
    "\n",
    "#### Example Usage\n",
    "\n",
    "The call `get_batch('train')` generates a batch of training data with `xb` and `yb` representing the input and target tensors, respectively. This function is essential for training the transformer model, as it ensures that data is prepared in the correct format and efficiently moved to the computation device."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "device: cuda\n"
     ]
    }
   ],
   "source": [
    "from typing import Literal, Tuple\n",
    "\n",
    "batch_size = 32 # batch_size -> how many independent sequences will we process in parallel?\n",
    "block_size = 8 # block_size -> the maximum context length for predictions\n",
    "\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "print(f\"device: {device}\")\n",
    "\n",
    "def get_batch(split: Literal['train', 'val']) -> Tuple[torch.Tensor, torch.Tensor]:\n",
    "    \"\"\"\n",
    "    Generate a small batch of data of inputs x and targets y\n",
    "    \"\"\"\n",
    "    data = train if split == 'train' else val  \n",
    "    ix = torch.randint(len(data) - block_size, (batch_size,)) \n",
    "    x = torch.stack([data[i:i+block_size] for i in ix])\n",
    "    y = torch.stack([data[i+1:i+block_size+1] for i in ix])\n",
    "    return x.to(device), y.to(device)\n",
    "\n",
    "xb, yb = get_batch('train')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Implementing the Head Class for Multi-Head Attention\n",
    "\n",
    "This section defines the `Head` class, a foundational component of a multi-head attention mechanism in transformer architectures. The class encapsulates the operations needed to compute scaled dot-product attention, including the linear transformations and masking necessary for autoregressive processing.\n",
    "\n",
    "#### Class Initialization\n",
    "\n",
    "The `Head` class inherits from `nn.Module`, and its `__init__` method sets up the primary components:\n",
    "\n",
    "- **`head_size`**: This defines the dimensionality of each attention head's output.\n",
    "- **`key`, `query`, and `value` Linear Layers**: These layers transform the input data into key, query, and value vectors, essential for calculating attention scores. Each transformation outputs vectors of size `head_size`.\n",
    "- **Lower Triangular Mask (`self.tril`)**: The attention mask, represented as a lower triangular matrix, ensures that each position in the sequence only attends to previous positions, which is crucial for autoregressive tasks like language modeling.\n",
    "- **Dropout Layer**: Dropout helps regularize the model by randomly dropping attention weights during training, reducing the likelihood of overfitting.\n",
    "\n",
    "#### Forward Method\n",
    "\n",
    "The `forward` method defines how data passes through the `Head` layer. Here’s a breakdown of each step:\n",
    "\n",
    "1. **Input Shape & Linear Transformations**: The input `x` of shape `(B, T, C)` (where `B` is the batch size, `T` is the sequence length, and `C` is the embedding size) is transformed into keys, queries, and values.\n",
    "   - `k`, `q`, and `v` represent the transformed key, query, and value matrices, respectively.\n",
    "\n",
    "2. **Attention Weight Calculation**:\n",
    "   - The query and key vectors are multiplied to compute the attention scores (`wei`), which are scaled by `sqrt(head_size)` for stability.\n",
    "   - The mask (`self.tril`) is applied to enforce autoregressive behavior, setting future positions to `-inf`, ensuring the model can only attend to previous positions in the sequence.\n",
    "   - `F.softmax` converts the attention scores into a probability distribution along the last dimension.\n",
    "\n",
    "3. **Attention Output**:\n",
    "   - The weighted attention matrix is calculated by performing a batch matrix multiplication (`wei @ v`), combining the attention scores with the values.\n",
    "   - Dropout is applied to the attention scores to further regularize the model.\n",
    "\n",
    "The output of this `Head` class is a weighted sum of values, where each value’s contribution depends on the computed attention weights, enabling the model to focus on relevant parts of the sequence based on the query and key representations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Head(nn.Module):\n",
    "    def __init__(self, head_size, embed_size, block_size, dropout=0.05):\n",
    "        super().__init__()\n",
    "        self.head_size = head_size\n",
    "        self.key = nn.Linear(embed_size, head_size, bias=False)\n",
    "        self.query = nn.Linear(embed_size, head_size, bias=False)\n",
    "        self.value = nn.Linear(embed_size, head_size, bias=False)\n",
    "        self.register_buffer('tril', torch.tril(torch.ones(block_size, block_size)))\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "        B, T, C = x.shape\n",
    "        k = self.key(x)\n",
    "        q = self.query(x)\n",
    "        v = self.value(x)\n",
    "        wei = q @ k.transpose(2, 1) / self.head_size ** 0.5\n",
    "        wei = wei.masked_fill(self.tril[:T, :T] == 0, float('-inf'))\n",
    "        wei = F.softmax(wei, dim=2)\n",
    "        wei = self.dropout(wei)\n",
    "        return wei @ v"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Demonstrating Masked Self-Attention with Triangular Matrix Masking\n",
    "\n",
    "This code snippet demonstrates a simplified version of a self-attention mechanism, using random tensors as `query` and `value` matrices. The purpose of the script is to show how attention weights are calculated and then masked with a triangular matrix to enforce causality, a key component in autoregressive tasks.\n",
    "\n",
    "#### Step-by-Step Breakdown\n",
    "\n",
    "1. **Initialize Query and Value Tensors**:\n",
    "   - `q` and `v` are tensors with random integer values, simulating the query and value representations from a batch with 1 sequence, 3 tokens, and 3-dimensional embeddings.\n",
    "   - Output:\n",
    "     ```\n",
    "     Query:\n",
    "     [[...]]\n",
    "     Value:\n",
    "     [[...]]\n",
    "     ```\n",
    "\n",
    "2. **Calculate Weights**:\n",
    "   - `wei = q @ v.transpose(2, 1) / 3 ** 0.5`: We compute attention weights by taking the dot product of the query and the transposed value tensor, then scale by the square root of the embedding size.\n",
    "   - The result is a matrix of attention weights for each pair of tokens.\n",
    "\n",
    "3. **Create and Apply the Mask**:\n",
    "   - `tril = torch.tril(torch.ones(3, 3))`: Creates a lower triangular matrix of ones, allowing each token to attend only to previous tokens and itself.\n",
    "   - `wei = wei.masked_fill(tril == 0, float('-inf'))`: Sets all positions above the main diagonal to `-inf` (using the mask) to prevent future tokens from being attended to, ensuring a causal (autoregressive) attention mechanism.\n",
    "\n",
    "4. **Apply Softmax to Weights**:\n",
    "   - `F.softmax(wei, dim=2)`: Converts the masked weights into probabilities, where any weight of `-inf` becomes 0 after the softmax operation. This step allows the model to weigh each token's importance proportionally while respecting causality.\n",
    "\n",
    "#### Example Output\n",
    "```plaintext\n",
    "Query:\n",
    " [[...]]\n",
    "Value:\n",
    " [[...]]\n",
    "weights:\n",
    " [[...]]\n",
    "Triangular Metrics:\n",
    " [[...]]\n",
    "Masked Weights:\n",
    " [[...]]\n",
    "Softmax ( e^-inf = 0 ):\n",
    " [[...]]\n",
    "```\n",
    "\n",
    "#### Importance\n",
    "\n",
    "The triangular mask is essential in autoregressive language models, ensuring each position in the sequence only attends to current and preceding positions. This technique prevents \"cheating\" by masking out future information, making it a fundamental part of sequence prediction and generation tasks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query:\n",
      " tensor([[[7, 3, 3],\n",
      "         [6, 2, 0],\n",
      "         [7, 6, 6]]])\n",
      "Value:\n",
      " tensor([[[6, 2, 4],\n",
      "         [7, 0, 4],\n",
      "         [2, 1, 0]]])\n",
      "weights:\n",
      " tensor([[[34.6410, 35.2184,  9.8150],\n",
      "         [23.0940, 24.2487,  8.0829],\n",
      "         [45.0333, 42.1466, 11.5470]]])\n",
      "Triangular Metrics:\n",
      " tensor([[1., 0., 0.],\n",
      "        [1., 1., 0.],\n",
      "        [1., 1., 1.]])\n",
      "Masked Weights\n",
      " tensor([[[34.6410,    -inf,    -inf],\n",
      "         [23.0940, 24.2487,    -inf],\n",
      "         [45.0333, 42.1466, 11.5470]]])\n",
      "Softmax ( e^-inf = 0 )\n",
      " tensor([[[1.0000e+00, 0.0000e+00, 0.0000e+00],\n",
      "         [2.3963e-01, 7.6037e-01, 0.0000e+00],\n",
      "         [9.4719e-01, 5.2812e-02, 2.7134e-15]]])\n"
     ]
    }
   ],
   "source": [
    "q = torch.randint(10, (1, 3, 3))\n",
    "v = torch.randint(10, (1, 3, 3))\n",
    "print(\"Query:\\n\",q)\n",
    "print(\"Value:\\n\",v)\n",
    "wei = q@v.transpose(2, 1)/3**0.5\n",
    "print(\"weights:\\n\", wei)\n",
    "tril = torch.tril(torch.ones(3, 3))\n",
    "print(\"Triangular Metrics:\\n\",tril)\n",
    "wei = wei.masked_fill(tril == 0, float('-inf'))\n",
    "print(\"Masked Weights\\n\", wei)\n",
    "print(\"Softmax ( e^-inf = 0 )\\n\", F.softmax(wei, dim=2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Multi-Head Attention Layer\n",
    "\n",
    "This code defines a `MultiHeadAttention` class, which is a core component in transformer architectures. Multi-head attention allows the model to focus on different parts of the input sequence simultaneously, capturing multiple types of relationships among tokens.\n",
    "\n",
    "#### Class Attributes\n",
    "\n",
    "- **`self.sa_head`**: This is a `ModuleList` containing multiple instances of the `Head` class, each of which performs a self-attention operation on the input. Each head learns a different attention pattern, capturing diverse relationships in the data.\n",
    "- **`self.proj`**: A linear layer that projects the concatenated outputs of all heads back to the original embedding size. This layer helps aggregate the multi-head attention information into a single tensor of the expected size.\n",
    "- **`self.dropout`**: A dropout layer applied after the projection, helping prevent overfitting by randomly setting some of the output units to zero during training.\n",
    "\n",
    "#### Forward Pass\n",
    "\n",
    "1. **Concatenation of Heads**:\n",
    "   - `torch.cat([head(x) for head in self.sa_head], dim=-1)`: The input `x` is passed through each head in `self.sa_head`, and their outputs are concatenated along the last dimension.\n",
    "   - This concatenation effectively gathers information from multiple attention patterns learned by each head.\n",
    "\n",
    "2. **Projection and Dropout**:\n",
    "   - `self.proj(x)`: The concatenated tensor is passed through the projection layer to transform it back to the embedding size, ensuring dimensional consistency.\n",
    "   - `self.dropout(...)`: Dropout is applied to the projected output, providing regularization to the model.\n",
    "\n",
    "#### Benefits of Multi-Head Attention\n",
    "\n",
    "By having multiple heads, the model can learn different aspects of the input sequence’s structure, such as positional relationships or dependencies between specific tokens. Each head learns its own query, key, and value transformations, enabling richer representations and enhancing the model's ability to capture complex dependencies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self, head_size, embed_size, block_size, num_head, dropout=0.05):\n",
    "        super().__init__()\n",
    "        self.sa_head = nn.ModuleList([Head(head_size, embed_size, block_size, dropout) for _ in range(num_head)])\n",
    "        self.proj = nn.Linear(embed_size, embed_size)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = torch.cat([head(x) for head in self.sa_head], dim=-1)\n",
    "        return self.dropout(self.proj(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Block Class: Transformer Block\n",
    "\n",
    "This code defines a `Block` class, which represents a single transformer block, an essential building block for transformer architectures like GPT or BERT. A transformer block consists of two key components: multi-head self-attention and a feed-forward network, each followed by a residual connection and layer normalization.\n",
    "\n",
    "#### Class Attributes\n",
    "\n",
    "1. **`self.multihead`**: This is an instance of the `MultiHeadAttention` class that performs multi-head self-attention. It allows the model to focus on different parts of the input sequence simultaneously.\n",
    "2. **`self.ff`**: This is an instance of the `FeedForward` class, which applies a simple feed-forward network to the output of the attention mechanism.\n",
    "3. **`self.ll1` and `self.ll2`**: These are Layer Normalization layers applied before each sub-layer (multi-head attention and feed-forward network). Layer normalization helps stabilize and speed up training by normalizing the input across the features.\n",
    "\n",
    "#### Forward Pass\n",
    "\n",
    "1. **Multi-Head Attention with Residual Connection**:\n",
    "   - `x = x + self.multihead(self.ll1(x))`: The input `x` is first passed through layer normalization (`self.ll1(x)`), then through the multi-head attention (`self.multihead(...)`), and finally a residual connection is added by summing the original `x` with the output of the attention mechanism.\n",
    "\n",
    "2. **Feed-Forward Network with Residual Connection**:\n",
    "   - `x = x + self.ff(self.ll2(x))`: The output from the previous step is passed through the second layer normalization (`self.ll2(x)`), followed by the feed-forward network (`self.ff(...)`). Again, a residual connection is added, allowing the original input to flow through the network.\n",
    "\n",
    "#### Summary of Components\n",
    "\n",
    "- **Multi-Head Attention**: Captures different relationships and dependencies in the input sequence.\n",
    "- **Feed-Forward Network**: Transforms the output from the attention mechanism, providing additional complexity and enabling the model to learn more intricate patterns.\n",
    "- **Layer Normalization**: Ensures stable and efficient training by normalizing the activations within each layer.\n",
    "- **Residual Connections**: Allow gradients to flow more easily through the network, improving training dynamics and helping prevent vanishing gradient problems.\n",
    "\n",
    "### Transformer Block Workflow\n",
    "In essence, this transformer block works as follows:\n",
    "1. Input is passed through the attention mechanism (after layer normalization).\n",
    "2. Residual connection is added between the original input and the output of the attention mechanism.\n",
    "3. The result is passed through the feed-forward network (after layer normalization).\n",
    "4. Another residual connection is added between the original input and the output of the feed-forward network.\n",
    "\n",
    "This modular structure is repeated multiple times in the transformer model, stacking several such blocks to create a deeper model capable of learning complex representations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FeedForward(nn.Module):\n",
    "    def __init__(self, embed_size, dropout=0.05):\n",
    "        super().__init__()\n",
    "        self.ff = nn.Sequential(\n",
    "            nn.Linear(embed_size, 4 * embed_size),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(4 * embed_size, embed_size),\n",
    "            nn.Dropout(dropout)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.ff(x)\n",
    "    \n",
    "class Block(nn.Module):\n",
    "    def __init__(self, head_size, embed_size, block_size, num_head, dropout=0.05):  # Reduced dropout\n",
    "        super().__init__()\n",
    "        head_size = embed_size // num_head\n",
    "        self.multihead = MultiHeadAttention(head_size, embed_size, block_size, num_head, dropout)\n",
    "        self.ff = FeedForward(embed_size, dropout)\n",
    "        self.ll1 = nn.LayerNorm(embed_size)\n",
    "        self.ll2 = nn.LayerNorm(embed_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x + self.multihead(self.ll1(x))\n",
    "        return x + self.ff(self.ll2(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LittleLanguageModel: Transformer-based Language Model\n",
    "\n",
    "The `LittleLanguageModel` class is a neural network architecture designed for language modeling. It uses a transformer-based structure with attention layers to predict the next word in a sequence. It incorporates several key components, including token embeddings, positional embeddings, and multiple transformer blocks stacked together.\n",
    "\n",
    "#### Key Components\n",
    "\n",
    "1. **Token Embedding Table**:\n",
    "   - `self.token_embedding_table = nn.Embedding(vocab_size, embed_size)`: This layer maps each token (word or character) in the vocabulary to a dense vector representation (embedding) of size `embed_size`.\n",
    "\n",
    "2. **Positional Embedding**:\n",
    "   - `self.positional_embedding = nn.Embedding(block_size, embed_size)`: This layer adds positional encodings to the input sequence, allowing the model to capture the order of the tokens, which is essential for language modeling.\n",
    "\n",
    "3. **Transformer Blocks**:\n",
    "   - `self.block = nn.Sequential(*[Block(head_size, embed_size, block_size, num_head, dropout) for _ in range(num_layers)])`: This is a stack of transformer blocks. Each block contains multi-head attention and feed-forward layers. The number of blocks (`num_layers`) can be adjusted.\n",
    "\n",
    "4. **Linear Layer**:\n",
    "   - `self.linear = nn.Linear(embed_size, vocab_size)`: After processing through the transformer blocks, the final embeddings are passed through a linear layer that projects the embedding space back to the vocabulary size. This generates logits, which can be used to calculate the probability of each token in the vocabulary.\n",
    "\n",
    "5. **Layer Normalization**:\n",
    "   - `self.layer_norm = nn.LayerNorm(embed_size)`: This layer normalizes the activations before passing them through the transformer blocks, helping improve training stability and convergence.\n",
    "\n",
    "#### Forward Pass\n",
    "\n",
    "The forward method defines how the input flows through the model:\n",
    "\n",
    "1. **Embedding Lookup**:\n",
    "   - `logits = self.token_embedding_table(idx)`: The input token indices (`idx`) are converted into token embeddings of size `embed_size`.\n",
    "\n",
    "2. **Positional Encoding**:\n",
    "   - `ps = self.positional_embedding(torch.arange(T, device=idx.device))`: Positional embeddings are added to the token embeddings to encode the order of the tokens in the sequence.\n",
    "\n",
    "3. **Transformer Blocks**:\n",
    "   - `x = logits + ps`: The token embeddings and positional encodings are summed.\n",
    "   - `logits = self.linear(self.layer_norm(self.block(x)))`: The combined representation is passed through the transformer blocks followed by layer normalization and a final linear transformation to predict the next token.\n",
    "\n",
    "4. **Loss Calculation**:\n",
    "   - If `targets` (ground truth labels) are provided, the model computes the cross-entropy loss between the predicted logits and the true targets.\n",
    "\n",
    "#### Generation (Sampling)\n",
    "\n",
    "The `generate` method generates text one token at a time, based on an initial input sequence `idx`.\n",
    "\n",
    "1. **Sampling Process**:\n",
    "   - `crop_idx = idx[:, -block_size:].to(device)`: The model keeps track of the last `block_size` tokens in the input sequence to make predictions.\n",
    "   - `logits, _ = self(crop_idx)`: The model generates logits (predicted token probabilities) for the next token.\n",
    "   - `logits = logits[:, -1, :] / temperature`: The logits are scaled by a `temperature` factor. A higher temperature results in more random sampling, while a lower temperature makes the model more confident.\n",
    "   - `probs = F.softmax(logits, dim=-1)`: The logits are converted to probabilities using the softmax function.\n",
    "   - `idx_next = torch.multinomial(probs, num_samples=1).to(device)`: The model samples the next token based on the probability distribution.\n",
    "   - `idx = torch.cat((idx, idx_next), dim=1)`: The generated token is appended to the input sequence, and the process repeats for `max_new_tokens` tokens.\n",
    "\n",
    "#### Summary\n",
    "\n",
    "- **Input**: The model takes in a sequence of token indices (`idx`) and generates predictions for the next token.\n",
    "- **Training**: During training, the model computes the cross-entropy loss between the predicted logits and the true token labels.\n",
    "- **Generation**: During generation, the model samples the next token based on the probabilities computed by the transformer and appends it to the input sequence.\n",
    "  \n",
    "The `LittleLanguageModel` uses a transformer architecture with multi-head attention to model language sequences and predict the next word or token. It allows for flexible training and text generation, making it suitable for various NLP tasks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LittleLanguageModel(nn.Module):\n",
    "    def __init__(self, vocab_size, head_size, embed_size, block_size, num_head, num_layers, dropout=0.05):  # Reduced dropout\n",
    "        super().__init__()\n",
    "        self.token_embedding_table = nn.Embedding(vocab_size, embed_size)\n",
    "        self.positional_embedding = nn.Embedding(block_size, embed_size)\n",
    "        self.linear = nn.Linear(embed_size, vocab_size)\n",
    "        self.block = nn.Sequential(*[Block(head_size, embed_size, block_size, num_head, dropout) for _ in range(num_layers)])\n",
    "        self.layer_norm = nn.LayerNorm(embed_size)\n",
    "\n",
    "    def forward(self, idx, targets=None):\n",
    "        B, T = idx.shape\n",
    "        logits = self.token_embedding_table(idx)  # (B, T, C)\n",
    "        ps = self.positional_embedding(torch.arange(T, device=idx.device))\n",
    "        x = logits + ps  # (B, T, C)\n",
    "        logits = self.linear(self.layer_norm(self.block(x)))  # Improved LayerNorm positioning\n",
    "        \n",
    "        loss = None\n",
    "        if targets is not None:\n",
    "            B, T, C = logits.shape\n",
    "            logits = logits.view(B * T, C)\n",
    "            targets = targets.view(B * T)\n",
    "            loss = F.cross_entropy(logits, targets)\n",
    "        \n",
    "        return logits, loss\n",
    "\n",
    "    def generate(self, idx, max_new_tokens, temperature=1.0):\n",
    "        if temperature <= 0.0:\n",
    "            raise ValueError(\"Temperature must be greater than zero for sampling.\")\n",
    "        \n",
    "        if temperature == 0.0:\n",
    "            return self(idx)[0].argmax(dim=-1)\n",
    "        \n",
    "        for _ in range(max_new_tokens):\n",
    "            crop_idx = idx[:, -block_size:].to(device)\n",
    "            logits, _ = self(crop_idx)\n",
    "            logits = logits[:, -1, :] / temperature\n",
    "            probs = F.softmax(logits, dim=-1)\n",
    "            idx_next = torch.multinomial(probs, num_samples=1).to(device)\n",
    "            idx = torch.cat((idx, idx_next), dim=1)\n",
    "        \n",
    "        return idx"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Setup and Initialization\n",
    "\n",
    "In this section, the transformer-based `LittleLanguageModel` is initialized with specific hyperparameters, followed by setting up the optimizer for training.\n",
    "\n",
    "#### Parameters Definition\n",
    "\n",
    "- **`embed_size = 512`**: This is the dimensionality of the token embeddings. A higher embedding size allows the model to capture more intricate relationships in the data but also increases the computational cost.\n",
    "  \n",
    "- **`num_head = 8`**: This specifies the number of attention heads in the multi-head attention mechanism. Multiple attention heads allow the model to attend to different parts of the input sequence simultaneously, improving its ability to learn diverse features.\n",
    "  \n",
    "- **`head_size = embed_size // num_head`**: The size of each attention head is calculated by dividing the `embed_size` by the number of heads. This ensures that the total embedding space is evenly split across the heads in the attention mechanism.\n",
    "  \n",
    "- **`num_layers = 6`**: The number of transformer layers stacked together. Each layer consists of multi-head attention followed by a feed-forward network. More layers can increase the model's capacity to learn complex representations but may also lead to overfitting and longer training times.\n",
    "  \n",
    "- **`vocab_size = len(vocab)`**: This defines the size of the vocabulary. It is determined by the number of unique characters or words in the dataset, which corresponds to the input and output space of the model.\n",
    "\n",
    "#### Model Instantiation\n",
    "\n",
    "- **`model = LittleLanguageModel(vocab_size, head_size, embed_size, block_size, num_head, num_layers).to(device)`**:\n",
    "   - This line creates an instance of the `LittleLanguageModel` with the parameters defined above.\n",
    "   - The model is moved to the device (`cuda` or `cpu`) based on availability of a GPU.\n",
    "\n",
    "#### Optimizer Setup\n",
    "\n",
    "- **`optimizer = torch.optim.AdamW(model.parameters(), lr=1e-3)`**:\n",
    "   - This line initializes the optimizer using **AdamW**, which is a variant of the Adam optimizer that includes weight decay to reduce overfitting.\n",
    "   - `model.parameters()` gives the optimizer access to all the trainable parameters of the model.\n",
    "   - The learning rate is set to `1e-3`, which is a commonly used value for training deep learning models.\n",
    "\n",
    "#### Summary\n",
    "\n",
    "This setup initializes the `LittleLanguageModel` with appropriate hyperparameters, followed by configuring the AdamW optimizer. The model and optimizer are now ready for training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the parameters\n",
    "embed_size = 512                    # Example embedding size\n",
    "num_head = 8                        # Example number of attention heads\n",
    "head_size = embed_size // num_head  # Example size of each attention head\n",
    "num_layers = 6                      # Example number of layers\n",
    "vocab_size = len(vocab)             # Example vocabulary size\n",
    "\n",
    "# Instantiate the model\n",
    "model = LittleLanguageModel(vocab_size, head_size, embed_size, block_size, num_head, num_layers).to(device)\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=1e-3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training the Little Language Model\n",
    "\n",
    "In this section, we define and execute the training process for the `LittleLanguageModel`. The `train_model` function handles both training and evaluation, updating the model weights based on the loss from the training data while periodically evaluating on the validation set.\n",
    "\n",
    "#### `train_model` Function Overview\n",
    "\n",
    "1. **Training and Validation Loss Tracking**:\n",
    "   - The function accepts the model, optimizer, and other hyperparameters such as the number of epochs (`epochs`) and evaluation interval (`eval_interval`).\n",
    "   - It tracks training and validation losses across epochs to monitor the performance of the model during training.\n",
    "\n",
    "2. **Epoch Loop**:\n",
    "   - For each epoch, the model is trained on the training data in mini-batches and periodically evaluated on the validation set.\n",
    "   - Every `eval_interval` epochs, the model is set to evaluation mode using `model.eval()` to disable dropout and other training-specific behaviors. The loss is then calculated for both the training and validation datasets over 100 batches.\n",
    "   - After evaluation, the model is set back to training mode using `model.train()`.\n",
    "\n",
    "3. **Loss Calculation**:\n",
    "   - For each batch, the model computes the output logits and compares them with the target labels to calculate the loss using **cross-entropy**.\n",
    "   - The loss for both the training and validation sets is accumulated over 100 batches, and the average is logged.\n",
    "\n",
    "4. **Gradient Computation and Parameter Update**:\n",
    "   - During the training phase, for each mini-batch, the model computes the forward pass, calculates the loss, and performs backpropagation (`loss.backward()`) to compute the gradients.\n",
    "   - The optimizer updates the model weights (`optimizer.step()`), applying the gradients.\n",
    "\n",
    "5. **Learning Rate Scheduling**:\n",
    "   - The learning rate is halved every 1000 epochs to gradually reduce the step size and stabilize training as the model approaches convergence.\n",
    "\n",
    "#### Training Execution\n",
    "\n",
    "- **`train_losses, val_losses = train_model(model, optimizer)`**: \n",
    "   - This line starts the training process and stores the resulting training and validation losses in `train_losses` and `val_losses`.\n",
    "   - These losses can later be plotted or analyzed to understand how well the model is converging over time.\n",
    "\n",
    "#### Summary\n",
    "\n",
    "This function trains the `LittleLanguageModel` over a specified number of epochs, computes the loss for both training and validation sets, and adjusts the learning rate as needed. It provides feedback every `eval_interval` epochs and tracks the model's performance to ensure that the learning process is progressing correctly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0 | Train Loss: 4.4753 | Val Loss: 4.4765\n",
      "Epoch 100 | Train Loss: 2.3680 | Val Loss: 2.3657\n",
      "Epoch 200 | Train Loss: 2.2707 | Val Loss: 2.2468\n",
      "Epoch 300 | Train Loss: 2.1758 | Val Loss: 2.1808\n",
      "Epoch 400 | Train Loss: 2.1586 | Val Loss: 2.1143\n",
      "Epoch 500 | Train Loss: 2.1263 | Val Loss: 2.1165\n",
      "Epoch 600 | Train Loss: 2.1242 | Val Loss: 2.1093\n",
      "Epoch 700 | Train Loss: 2.0875 | Val Loss: 2.0807\n",
      "Epoch 800 | Train Loss: 2.1040 | Val Loss: 2.0884\n",
      "Epoch 900 | Train Loss: 2.0877 | Val Loss: 2.0961\n",
      "Epoch 1000 | Train Loss: 2.0802 | Val Loss: 2.0652\n",
      "Epoch 1100 | Train Loss: 2.0257 | Val Loss: 1.9979\n",
      "Epoch 1200 | Train Loss: 2.0309 | Val Loss: 2.0138\n",
      "Epoch 1300 | Train Loss: 2.0171 | Val Loss: 2.0232\n",
      "Epoch 1400 | Train Loss: 1.9927 | Val Loss: 1.9773\n",
      "Epoch 1500 | Train Loss: 2.0052 | Val Loss: 1.9843\n",
      "Epoch 1600 | Train Loss: 2.0044 | Val Loss: 1.9860\n",
      "Epoch 1700 | Train Loss: 1.9923 | Val Loss: 1.9733\n",
      "Epoch 1800 | Train Loss: 1.9759 | Val Loss: 1.9874\n",
      "Epoch 1900 | Train Loss: 1.9868 | Val Loss: 1.9954\n",
      "Epoch 2000 | Train Loss: 1.9930 | Val Loss: 1.9732\n",
      "Epoch 2100 | Train Loss: 1.9542 | Val Loss: 1.9244\n",
      "Epoch 2200 | Train Loss: 1.9436 | Val Loss: 1.9382\n",
      "Epoch 2300 | Train Loss: 1.9228 | Val Loss: 1.9258\n",
      "Epoch 2400 | Train Loss: 1.9192 | Val Loss: 1.9335\n",
      "Epoch 2500 | Train Loss: 1.9317 | Val Loss: 1.9147\n",
      "Epoch 2600 | Train Loss: 1.9264 | Val Loss: 1.9081\n",
      "Epoch 2700 | Train Loss: 1.9009 | Val Loss: 1.9078\n",
      "Epoch 2800 | Train Loss: 1.9269 | Val Loss: 1.9228\n",
      "Epoch 2900 | Train Loss: 1.9066 | Val Loss: 1.9224\n",
      "Epoch 3000 | Train Loss: 1.9246 | Val Loss: 1.9106\n",
      "Epoch 3100 | Train Loss: 1.8946 | Val Loss: 1.8800\n",
      "Epoch 3200 | Train Loss: 1.9017 | Val Loss: 1.8788\n",
      "Epoch 3300 | Train Loss: 1.8908 | Val Loss: 1.8789\n",
      "Epoch 3400 | Train Loss: 1.8932 | Val Loss: 1.8777\n",
      "Epoch 3500 | Train Loss: 1.8839 | Val Loss: 1.8649\n",
      "Epoch 3600 | Train Loss: 1.8915 | Val Loss: 1.8951\n",
      "Epoch 3700 | Train Loss: 1.8896 | Val Loss: 1.8751\n",
      "Epoch 3800 | Train Loss: 1.8714 | Val Loss: 1.8722\n",
      "Epoch 3900 | Train Loss: 1.8797 | Val Loss: 1.8613\n",
      "Epoch 4000 | Train Loss: 1.8902 | Val Loss: 1.8602\n",
      "Epoch 4100 | Train Loss: 1.8689 | Val Loss: 1.8601\n",
      "Epoch 4200 | Train Loss: 1.8587 | Val Loss: 1.8567\n",
      "Epoch 4300 | Train Loss: 1.8718 | Val Loss: 1.8480\n",
      "Epoch 4400 | Train Loss: 1.8732 | Val Loss: 1.8358\n",
      "Epoch 4500 | Train Loss: 1.8536 | Val Loss: 1.8491\n",
      "Epoch 4600 | Train Loss: 1.8514 | Val Loss: 1.8467\n",
      "Epoch 4700 | Train Loss: 1.8521 | Val Loss: 1.8438\n",
      "Epoch 4800 | Train Loss: 1.8704 | Val Loss: 1.8576\n",
      "Epoch 4900 | Train Loss: 1.8548 | Val Loss: 1.8517\n"
     ]
    }
   ],
   "source": [
    "def train_model(model, optimizer, epochs=5000, eval_interval=100):\n",
    "    train_losses = []\n",
    "    val_losses = []\n",
    "    for epoch in range(epochs):\n",
    "        if epoch % eval_interval == 0:\n",
    "            model.eval()\n",
    "            train_loss, val_loss = 0.0, 0.0\n",
    "            with torch.no_grad():\n",
    "                for _ in range(100):\n",
    "                    x_train, y_train = get_batch('train')\n",
    "                    x_val, y_val = get_batch('val')\n",
    "                    _, train_batch_loss = model(x_train, y_train)\n",
    "                    _, val_batch_loss = model(x_val, y_val)\n",
    "                    train_loss += train_batch_loss.item()\n",
    "                    val_loss += val_batch_loss.item()\n",
    "            train_losses.append(train_loss / 100)\n",
    "            val_losses.append(val_loss / 100)\n",
    "            print(f\"Epoch {epoch} | Train Loss: {train_loss / 100:.4f} | Val Loss: {val_loss / 100:.4f}\")\n",
    "            model.train()\n",
    "\n",
    "        x_batch, y_batch = get_batch('train')\n",
    "        optimizer.zero_grad(set_to_none=True)\n",
    "        _, loss = model(x_batch, y_batch)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        # Learning rate scheduler (e.g., halve every 1000 epochs)\n",
    "        if epoch % 1000 == 0 and epoch > 0:\n",
    "            for g in optimizer.param_groups:\n",
    "                g['lr'] *= 0.5\n",
    "    return train_losses, val_losses\n",
    "\n",
    "# Train the model\n",
    "train_losses, val_losses = train_model(model, optimizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plotting the Training and Validation Losses\n",
    "\n",
    "The code snippet provided generates a plot that visualizes the training and validation losses over time, helping to track the model's learning progress and identify potential overfitting or underfitting. \n",
    "\n",
    "#### `plot_losses` Function Overview\n",
    "\n",
    "1. **Plot Setup**:\n",
    "   - The function begins by creating a figure with a specified size using `plt.figure(figsize=(10, 5))`.\n",
    "   - Then, it plots both the training and validation losses using `plt.plot(train_losses)` and `plt.plot(val_losses)`, respectively. These losses are tracked over the evaluation intervals.\n",
    "   \n",
    "2. **Labels and Legends**:\n",
    "   - The `xlabel` and `ylabel` set the axes to indicate the number of evaluation intervals and the loss values, respectively.\n",
    "   - `plt.legend()` adds a legend to distinguish between the training and validation loss curves.\n",
    "   \n",
    "3. **Title and Display**:\n",
    "   - The plot is titled \"Training and Validation Loss over Time\" using `plt.title()`, providing context for the graph.\n",
    "   - Finally, `plt.show()` displays the plot.\n",
    "\n",
    "#### Visualizing the Losses\n",
    "\n",
    "By calling `plot_losses(train_losses, val_losses)`, you will be able to see how the model's performance evolves. If the training loss decreases while the validation loss increases significantly, it could be a sign of overfitting. On the other hand, if both losses decrease gradually, the model is likely generalizing well.\n",
    "\n",
    "This visualization is useful for evaluating the effectiveness of your training process and tuning your model's hyperparameters accordingly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1000x500 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def plot_losses(train_losses, val_losses):\n",
    "    plt.figure(figsize=(10, 5))\n",
    "    plt.plot(train_losses, label='Training Loss')\n",
    "    plt.plot(val_losses, label='Validation Loss')\n",
    "    plt.xlabel('Evaluation Interval')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.legend()\n",
    "    plt.title('Training and Validation Loss over Time')\n",
    "    plt.show()\n",
    "\n",
    "# Plot the losses\n",
    "plot_losses(train_losses, val_losses)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generating Text Using the Model\n",
    "\n",
    "The provided code snippet demonstrates how to generate text using a trained language model, starting from a given input sequence. Here's a breakdown of how the text generation works:\n",
    "\n",
    "#### 1. **Encoding the Input**:\n",
    "```python\n",
    "ids = torch.tensor(encode(\"Nel mezzo del cammin di nostra vita\"), dtype=torch.long).unsqueeze(0).to(device)\n",
    "```\n",
    "- **Input**: `\"Nel mezzo del cammin di nostra vita\"` is the starting text (in Italian).\n",
    "- The text is encoded into integer tokens using the `encode` function, which maps each character in the string to its corresponding index in the vocabulary.\n",
    "- `.unsqueeze(0)` adds a batch dimension to the tensor (since the model expects a batch of sequences).\n",
    "- `.to(device)` ensures that the tensor is placed on the appropriate device (either CPU or GPU).\n",
    "\n",
    "#### 2. **Generating New Tokens**:\n",
    "```python\n",
    "generated_ids = model.generate(ids, max_new_tokens=2000, temperature=1.0)\n",
    "```\n",
    "- **Text Generation**: The `generate` method of the `LittleLanguageModel` generates a sequence of new tokens based on the initial input `ids`.\n",
    "- **Parameters**:\n",
    "  - `max_new_tokens=2000`: Specifies that up to 2000 new tokens should be generated.\n",
    "  - `temperature=1.0`: Controls the randomness of the predictions. A temperature of 1.0 means the model will sample from the distribution without scaling it (more random behavior). A lower temperature would make the model more deterministic, while a higher temperature would make it more random.\n",
    "\n",
    "#### 3. **Decoding and Printing the Generated Text**:\n",
    "```python\n",
    "print(decode(generated_ids[0].tolist()))\n",
    "```\n",
    "- The generated token IDs are decoded back into human-readable text using the `decode` function. The `tolist()` converts the tensor to a Python list for decoding.\n",
    "- Finally, the generated text is printed.\n",
    "\n",
    "This will output a continuation of the initial input text, produced by the model based on the learned patterns and context."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Nel mezzo del cammin di nostra vita,\n",
      "\n",
      "gensarà sona.\n",
      "\n",
      "  Que sù di condò a l'a spoi sando tan duce le ne si genteri di Coi: \"O vote,\n",
      "\n",
      "confella,\n",
      "\n",
      "  riviglio, sazza\n",
      "\n",
      "piel si monace,\n",
      "\n",
      "ha e divo acramori!\";\n",
      "\n",
      "  e Dietra, anota cal que le raggio memme diria, de lo fa testo con dura cio teme la liette savigne di iolo\n",
      "\n",
      "che Giacetto.\n",
      "\n",
      "  O e tra  tu tutto,\n",
      "\n",
      "quel de questò\n",
      "\n",
      "che quellio vegno\n",
      "\n",
      "\"Mace an il sento più la precro e non l'orto porto\n",
      "\n",
      "la che la de griato: \"Per la l'accaritta, l'amma chiuna;\n",
      "\n",
      "e Ma dondi, into la la tus vi paeler lo gronda, e dità, cir avi sura lucatorno è valira, segi terno\n",
      "\n",
      "dendo la certo tutti avresse, non le le cal farlo cietra odel sanza;\n",
      "\n",
      "  'l guardo\n",
      "\n",
      "dostro al palente e luri.\n",
      "\n",
      "  Nol sembran hi del mate a contore,\n",
      "\n",
      "  comma lo mondo, a villa\n",
      "\n",
      "che d'al ginti;\n",
      "\n",
      "  se rigoglio, quel da;\n",
      "\n",
      "oco quiferta,\n",
      "\n",
      "mor s'marebbe;\n",
      "\n",
      "  umaede,\n",
      "\n",
      "commo seso, tu poi, né nostapenti\n",
      "\n",
      "a già, che e 'l titpasco.\n",
      "\n",
      "  E per la pioci di quele che più intando a n'alla nostritre anche storichi Biaronò '            \n",
      "e ltre 'nvere\n",
      "\n",
      "con soglia\n",
      "\n",
      "porto viva,\n",
      "\n",
      "redera, quel comanto.\n",
      "\n",
      "  Sa fiava le pia prea reggiar, il trapea\n",
      "\n",
      "ch'' trossa suo giova giglero guando Ma ressere,\n",
      "\n",
      "cé in forse,\n",
      "\n",
      "menta fialorra adra\n",
      "\n",
      "'l conde,\n",
      "\n",
      "  granerda partel costo sontra puzzaio che 'l mano taratterbito\n",
      "\n",
      "lece che in lettie dalvessa.\n",
      "\n",
      "\n",
      "\n",
      "colpitote, aper milli che t'io ch'io us to\".\n",
      "\n",
      "  Allio acque.\n",
      "\n",
      "  Giù del mio ch'ei sù nostia alemin mer temprona,\n",
      "\n",
      "gittento son siogne le novea per ché 'l giano,\n",
      "\n",
      "cerguessante,\n",
      "\n",
      "e rolore, si vede\n",
      "\n",
      "   tanto in scen da semmoi e d'etro il pertò,\n",
      "\n",
      "e già manta vis di tarmo, e ma cita vuome le s'alame fu morte o che de l'or togno, verdoa Coi\".\n",
      "\n",
      "  Coiasero; \"Ché, ur d'ustizza di fiare,\n",
      "\n",
      "e vaggio\n",
      "\n",
      "ol che nosì diravi,\n",
      "\n",
      "i qua lo sì che l'al dero in tagnorsa?\n",
      "\n",
      "petate fra\".\n",
      "\n",
      "  Crai ch'a l'aloto\n",
      "\n",
      "di la per sù genti spiorte bilglie\n",
      "\n",
      "che l'unte, quanno,\n",
      "\n",
      "palte\n",
      "\n",
      "ch'io m'un vedera che, tu non la potta de la tara traghi\n",
      "\n",
      "   Qual tutto dinno.\n",
      "\n",
      "\n",
      "  Quanda fii de mente vente or che ioloi,\n",
      "\n",
      "ne là che l'undo e iommo par cottirti,\n",
      "\n",
      "non può\n"
     ]
    }
   ],
   "source": [
    "ids = torch.tensor(encode(\"Nel mezzo del cammin di nostra vita\"), dtype=torch.long).unsqueeze(0).to(device)\n",
    "generated_ids = model.generate(ids, max_new_tokens=2000, temperature=0.8)\n",
    "\n",
    "print(decode(generated_ids[0].tolist()))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
